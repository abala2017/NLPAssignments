{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Program 1\n",
    "## 1.a\n",
    "def getDicts(filename): ## create the function, takes in the training filename\n",
    "    \n",
    "    ## 1.b\n",
    "    f = open(filename, \"r\", encoding='utf-8') ## open the file\n",
    "    text = f.read() ## read the entire contents of the file\n",
    "    f.close() ## close the file\n",
    "    text.replace('\\n', '') ## remove the newlines\n",
    "    \n",
    "    ## 1.c\n",
    "    tokens = word_tokenize(text) ## tokenize the words\n",
    "    \n",
    "    ## 1.d\n",
    "    bigrams = list(ngrams(tokens, 2)) ## create the list of bigrams\n",
    "    \n",
    "    ## 1.e\n",
    "    unigrams = tokens ## create the list of unigrams (basically just the tokens)\n",
    "    \n",
    "    ## 1.f\n",
    "    bigram_dict = {b: bigrams.count(b) for b in set(bigrams)} ## create the count dict for bigrams\n",
    "    \n",
    "    ## 1.g\n",
    "    unigram_dict = {t: unigrams.count(t) for t in set(unigrams)} ## create the count dict for unigrams\n",
    "    \n",
    "    ## 1.h\n",
    "    return unigram_dict, bigram_dict ## return the bigram and unigram dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Program 2\n",
    "def getPreds(filename, outputfile):\n",
    "    \n",
    "    ## 2.a\n",
    "    ## unpickle the dictionaries and load them in\n",
    "    uni_eng = pickle.load(open('uni_eng.p', 'rb'))\n",
    "    bi_eng = pickle.load(open('bi_eng.p', 'rb'))\n",
    "    uni_fr = pickle.load(open('uni_fr.p', 'rb'))\n",
    "    bi_fr = pickle.load(open('bi_fr.p', 'rb'))\n",
    "    uni_it = pickle.load(open('uni_it.p', 'rb'))\n",
    "    bi_it = pickle.load(open('bi_it.p', 'rb'))\n",
    "    \n",
    "    ## 2.b\n",
    "    f = open(filename, 'r', encoding = 'utf-8') ## open the file with t\n",
    "    line = f.readline() ## read the first line\n",
    "    preds = [] ## list to hold the predictions\n",
    "    while(line): ## go by line to predict\n",
    "        \n",
    "        uni = word_tokenize(line) ## tokenize the line \n",
    "        bi = list(ngrams(uni, 2))  ## get the bigrams from the line\n",
    "        \n",
    "        ## initialize the laplace values\n",
    "        laplace_eng = 1\n",
    "        laplace_fr = 1\n",
    "        laplace_it = 1\n",
    "        \n",
    "        for bigram in bi: ## go through the bigrams in the line\n",
    "            \n",
    "            \n",
    "            b_eng = bi_eng[bigram] if bigram in bi_eng else 0 ## get the number of bigram occurrences in the training\n",
    "            u_eng = uni_eng[bigram[0]] if bigram[0] in uni_eng else 0 ## get the number of unigram occurrences in the training\n",
    "            v_eng  =  len(uni_eng) ## get the size of the vocab\n",
    "            \n",
    "            b_fr = bi_fr[bigram] if bigram in bi_fr else 0 ## get the number of bigram occurences in the training\n",
    "            u_fr = uni_fr[bigram[0]] if bigram[0] in uni_fr else 0 ## get the number of unigram occurrences in the training\n",
    "            v_fr  =  len(uni_fr) ## get the size of the vocab\n",
    "            \n",
    "            b_it = bi_it[bigram] if bigram in bi_it else 0 ## get the number of bigram occurences in the training\n",
    "            u_it = uni_it[bigram[0]] if bigram[0] in uni_it else 0 ## get the number of unigram occurrences in the training\n",
    "            v_it  =  len(uni_it) ## get the size of the vocab\n",
    "            \n",
    "            ## Use the laplace equation to update the value based on the current bigram\n",
    "            laplace_eng = laplace_eng * ((b_eng + 1) / (u_eng + v_eng))\n",
    "            laplace_fr = laplace_fr * ((b_fr + 1) / (u_fr + v_fr))\n",
    "            laplace_it = laplace_it * ((b_it + 1) / (u_it + v_it))\n",
    "\n",
    "        ## Find the highest probability language based on the laplace value for each language\n",
    "        if(laplace_eng > laplace_fr and laplace_eng > laplace_it):\n",
    "            preds.append(\"English\") ## append the predicition to the list\n",
    "        elif (laplace_fr > laplace_it):\n",
    "            preds.append(\"French\") ## append the predicition to the list\n",
    "        else:\n",
    "            preds.append(\"Italian\") ## append the predicition to the list\n",
    "        \n",
    "        line = f.readline() ## go to the next line\n",
    "    f.close() ## close the file\n",
    "    \n",
    "    f = open(outputfile, 'w') ## open the output file\n",
    "    \n",
    "    for i in range(len(preds)): ## iterate through the predictions\n",
    "        line = str(i + 1) + ' ' + preds[i] + '\\n' ## create the string\n",
    "        f.write(line) ## write the line to the file\n",
    "        \n",
    "    f.close() ## close the file\n",
    "    \n",
    "        \n",
    "    return(preds) ## return the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.b\n",
    "def getAccuracy(preds, filename): \n",
    "    \n",
    "    f = open(filename, 'r', encoding = 'utf-8') ## open the actual language file\n",
    "    actual = f.read() ## read all the data\n",
    "    actual = ''.join([i for i in actual if not i.isdigit()]) ## remove the digits\n",
    "    actual = actual.replace(' ', '').split('\\n') ## remove the spaces and make it a list for easy comparison\n",
    "    \n",
    "    correct = 0 ## count the correct predictions\n",
    "    incorrect = [] ## keep track of the incorrect line numbers\n",
    "    for i in range(len(preds)): ## iterate through the predictions\n",
    "        if preds[i] == actual[i]: ## check if pred = actual\n",
    "            correct = correct + 1 ## add 1 to correct if pred == actual\n",
    "        else:\n",
    "            incorrect.append(i + 1) ## else add the line to the incorrect lines\n",
    "            \n",
    "    accuracy = correct/len(preds) ## calculate the accuracy\n",
    "\n",
    "    return accuracy, incorrect ## return the accuracy and the incorrect lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ## i\n",
    "    uni_eng, bi_eng = getDicts('ngram_files/LangId.train.English') ## create the english dicts\n",
    "    #print(\"English Completed\")\n",
    "    uni_fr, bi_fr = getDicts('ngram_files/LangId.train.French') ## create the french dicts\n",
    "    #print(\"French Completed\")\n",
    "    uni_it, bi_it = getDicts('ngram_files/LangId.train.Italian') ## create the italian dicts\n",
    "    #print(\"Italian Completed\")\n",
    "    \n",
    "    ## pickle all the created dictionaries\n",
    "    pickle.dump(uni_eng, open(\"uni_eng.p\",\"wb\"))\n",
    "    pickle.dump(bi_eng, open(\"bi_eng.p\",\"wb\"))\n",
    "    pickle.dump(uni_fr, open(\"uni_fr.p\",\"wb\"))\n",
    "    pickle.dump(bi_fr, open(\"bi_fr.p\",\"wb\"))\n",
    "    pickle.dump(uni_it, open(\"uni_it.p\",\"wb\"))\n",
    "    pickle.dump(bi_it, open(\"bi_it.p\",\"wb\"))\n",
    "    \n",
    "    preds = getPreds('ngram_files/LangId.test', 'output.txt') ## get the prediction and write them\n",
    "    accuracy, incorrect = getAccuracy(preds, 'ngram_files/LangId.sol') ## get the accuracy and incorrect lines\n",
    "    ## 2.c\n",
    "    print(\"The accuracy we get is \" + str(accuracy)) ## outpt the accuracy\n",
    "    print(\"We have incorrect classifications on the following lines :\") ## output the incorrect lines\n",
    "    for i in incorrect:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
