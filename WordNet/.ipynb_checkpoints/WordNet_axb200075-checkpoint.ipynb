{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arjun Balasubramanian\n",
    "axb200075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.collocations import *\n",
    "import math\n",
    "from nltk.book import text4\n",
    "#nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "Wordnet Summary:\n",
    "Wordnet is database which holds relations between words. The relations tell us how words relate to each other with them being synonyms, hyponyms, and more. In addition each word has details reguarding defnition, part of speech, and other details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('rabbit.n.01'), Synset('lapin.n.01'), Synset('rabbit.n.03'), Synset('rabbit.v.01')]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "word = \"rabbit\" ## select noun\n",
    "synsets = wn.synsets(word) ## get the synsets\n",
    "print(synsets) ## output the synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbit : any of various burrowing animals of the family Leporidae having long ears and short tails; some domesticated and raised for pets or food\n",
      "\n",
      "\n",
      "examples :\n",
      "\n",
      "\n",
      "lemmas : \n",
      "Lemma('rabbit.n.01.rabbit')\n",
      "Lemma('rabbit.n.01.coney')\n",
      "Lemma('rabbit.n.01.cony')\n",
      "\n",
      "\n",
      "Traversing up we get :\n",
      "Synset('leporid.n.01')\n",
      "Synset('lagomorph.n.01')\n",
      "Synset('placental.n.01')\n",
      "Synset('mammal.n.01')\n",
      "Synset('vertebrate.n.01')\n",
      "Synset('chordate.n.01')\n",
      "Synset('animal.n.01')\n",
      "Synset('organism.n.01')\n",
      "Synset('living_thing.n.01')\n",
      "Synset('whole.n.02')\n",
      "Synset('object.n.01')\n",
      "Synset('physical_entity.n.01')\n",
      "Synset('entity.n.01')\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "synset = synsets[0] ## choose a synset\n",
    "\n",
    "print(word + \" : \"+ synset.definition()) ## output the definition\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"examples\" + \" :\")\n",
    "for ex in synset.examples(): ## iterate through the examples\n",
    "    print(ex) ## output the example\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"lemmas\" + \" : \")\n",
    "for lem in synset.lemmas(): ## iterate through the lemmas\n",
    "    print(lem) ## output the lemma\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Traversing up we get :\")\n",
    "while len(synset.hypernyms()) > 0: ## terminating condition is when the hypernym list has a length of 0\n",
    "    print (synset.hypernyms()[0]) ## print the hypernym\n",
    "    synset = synset.hypernyms()[0] ## set the current noun to the hypernym to keep iterating up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "The way that the nouns are organized is that all nouns branch out from the all encompassing noun, entity. They are all branched in a way such that the upper branches are hypernyms of the lower branches and the lower branches are hyponyms of the upper branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypernyms : \n",
      "\n",
      "[Synset('increase.n.02')]\n",
      "\n",
      "hyponyms : \n",
      "\n",
      "[Synset('quantum_leap.n.01')]\n",
      "\n",
      "meronyms : \n",
      "\n",
      "[]\n",
      "\n",
      "holonyms : \n",
      "\n",
      "[]\n",
      "\n",
      "antonym : \n",
      "\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "synset = synsets[0] ## choose one synset\n",
    "\n",
    "print(\"hypernyms : \\n\")\n",
    "print(synset.hypernyms()) ## output the hypernyms\n",
    "print()\n",
    "\n",
    "print(\"hyponyms : \\n\")\n",
    "print(synset.hyponyms()) ## output the hyponyms\n",
    "print()\n",
    "\n",
    "print(\"meronyms : \\n\")\n",
    "print(synset.part_meronyms()) ## output the meronyms\n",
    "print()\n",
    "\n",
    "print(\"holonyms : \\n\")\n",
    "print(synset.part_holonyms()) ## output the holonyms\n",
    "print()\n",
    "\n",
    "print(\"antonym : \\n\")\n",
    "for ant in synset.lemmas(): ## get the lemmas\n",
    "    print(ant.antonyms()) ## get the antonym from the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('jump.n.01'), Synset('leap.n.02'), Synset('jump.n.03'), Synset('startle.n.01'), Synset('jump.n.05'), Synset('jump.n.06'), Synset('jump.v.01'), Synset('startle.v.02'), Synset('jump.v.03'), Synset('jump.v.04'), Synset('leap_out.v.01'), Synset('jump.v.06'), Synset('rise.v.11'), Synset('jump.v.08'), Synset('derail.v.02'), Synset('chute.v.01'), Synset('jump.v.11'), Synset('jumpstart.v.01'), Synset('jump.v.13'), Synset('leap.v.02'), Synset('alternate.v.01')]\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "word = \"jump\" ## select the verb\n",
    "synsets = wn.synsets(word) ## get all synsets\n",
    "print(synsets) ## print all synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump : move or jump suddenly, as if in surprise or alarm\n",
      "\n",
      "\n",
      "examples :\n",
      "She startled when I walked into the room\n",
      "\n",
      "\n",
      "lemmas : \n",
      "Lemma('startle.v.02.startle')\n",
      "Lemma('startle.v.02.jump')\n",
      "Lemma('startle.v.02.start')\n",
      "\n",
      "\n",
      "Traversing up we get :\n",
      "Synset('move.v.03')\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "synset = synsets[7] ## choose a synset\n",
    "\n",
    "print(word + \" : \"+ synset.definition()) ## output the definiton\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"examples\" + \" :\")\n",
    "for ex in synset.examples(): ## iterate through the examples\n",
    "    print(ex) ## output the example\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"lemmas\" + \" : \")\n",
    "for lem in synset.lemmas(): ## iterate through the lemmas\n",
    "    print(lem) ## output the lemma\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Traversing up we get :\")\n",
    "while len(synset.hypernyms()) > 0: ## terminating condition when the hypernym list has a len of 0\n",
    "    print (synset.hypernyms()[0]) ## output the hypernym\n",
    "    synset = synset.hypernyms()[0] ## traverse up by setting the current noun to the hypernym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "Verbs in Wordnet are organized into separate hierarchies. The way these work is that there is a general verb term at the top and more specific as they get down with the upper branches being hypernyms of the lower branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "wn.morphy(word,wn.VERB) ## get the different forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wu-Palmer similarity between lunch and dinner is 0.875\n",
      "The lesk choice for the sentenct : \n",
      "I like to iron my clothes after washing them\n",
      "is Synset('iron.n.04')\n",
      "which has a defintion of : \n",
      "home appliance consisting of a flat metal base that is heated and used to smooth cloth\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "word1 = 'lunch' ## Word one\n",
    "word2 = 'dinner' ## Word two\n",
    "synset1 = wn.synsets(word1)[0] ## get the first synset\n",
    "synset2 = wn.synsets(word2)[0] ## get the second synset\n",
    "\n",
    "wuPalmer = synset1.wup_similarity(synset2) ## get the Wu Palmer metric\n",
    "print(\"The Wu-Palmer similarity between \" + word1 + \" and \" + word2 + \" is \" + str(wuPalmer)) ## output the Wu-Palmer\n",
    "\n",
    "sent = \"I like to iron my clothes after washing them\" ## set a sentence\n",
    "sent_token = sent.split(' ') ## split it into the tokens\n",
    "lesk_val = lesk(sent, 'iron', 'n') ## gets the correct form of iron, the verb form\n",
    "print(\"The lesk choice for the sentenct : \")\n",
    "print(sent)\n",
    "print(\"is \" + str(lesk_val))\n",
    "print(\"which has a defintion of : \")\n",
    "print(lesk_val.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "The Wu-Palmer similarity between lunch and dinner should be pretty similar as they are similar words since they are both meals. The .875 makes sense as a value of 1 would mean they are the same. The lesk algorithm gives us the correct iron as it is the one that appears in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The synset Synset('hate.n.01') has a positive score of 0.125 has a negative score of 0.375 has an objective score of 0.5\n",
      "\n",
      "The synset Synset('hate.v.01') has a positive score of 0.0 has a negative score of 0.75 has an objective score of 0.25\n",
      "\n",
      "I : pos = 0.0, neg = 0.0, obj = 1.0\n",
      "love : pos = 0.625, neg = 0.0, obj = 0.375\n",
      "learning : pos = 0.0, neg = 0.0, obj = 1.0\n",
      "about : pos = 0.0, neg = 0.0, obj = 1.0\n",
      "natural : pos = 0.0, neg = 0.0, obj = 1.0\n",
      "language : pos = 0.0, neg = 0.0, obj = 1.0\n",
      "processing : pos = 0.25, neg = 0.0, obj = 0.75\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "charged_word = 'hate' ## set a charged word\n",
    "senti_synsets = list(swn.senti_synsets(charged_word)) ## get the senti-synsets\n",
    "for senti_synset in senti_synsets: ## iterate through the senti-synsets\n",
    "    print(\"The synset \" + str(senti_synset.synset) +\n",
    "          \" has a positive score of \" + str(senti_synset.pos_score()) +\n",
    "          \" has a negative score of \" + str(senti_synset.neg_score()) +\n",
    "          \" has an objective score of \" + str(senti_synset.obj_score())\n",
    "         ) ## output the scores of the synsets\n",
    "    print()\n",
    "\n",
    "sent = 'I love learning about natural language processing' ## make a sentence\n",
    "for word in sent.split(' '): ## iterate through the words in the sentence\n",
    "    sentinet = list(swn.senti_synsets(word)) ## convert the output to a list\n",
    "    if(len(sentinet) > 0): ## if the list has elements\n",
    "        sentinet = sentinet[0] ## get the first sentinet\n",
    "        print(word + \n",
    "              ' : pos = ' + str(sentinet.pos_score()) +\n",
    "              ', neg = ' + str(sentinet.neg_score()) +\n",
    "              ', obj = ' + str(sentinet.obj_score())\n",
    "             ) ## output the sentiment of the word\n",
    "    else:\n",
    "        print(word + ' does not have a SentiWordNet') ## if there is no sentinet for the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "These scores correlate quite well with the words as the more emotionally charged words have higher sentiment scores. One thing I found odd was that love didn't have a higer positive sentiment. These would be helpful in many applications as it can tell you how the writer of the text feels about the subject of the sentence. For example you could use the average sentiment of the sentence to figure out whether reviews are positive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n",
      "\n",
      "The point-wise mutual information value is 8.174006563041724\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "text4.collocations() ## output the collocations\n",
    "coallocation = 'fellow citizens' ## choose one collocation\n",
    "num_bigrams = len(text4.tokens)-1 ## get the number of bigrams\n",
    "pxy = \" \".join(text4.tokens).count(coallocation) / num_bigrams ## get p(x,y)\n",
    "px = text4.count(coallocation.split(' ')[0]) / len(text4.tokens) ## get p(x)\n",
    "py = text4.count(coallocation.split(' ')[1]) / len(text4.tokens) ## get p(y)\n",
    "pmi = math.log(pxy / (px * py) , 2) ## get the point-wise mutual information\n",
    "print()\n",
    "print(\"The point-wise mutual information value is \" + str(pmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10\n",
    "Collocations are two words that have a higher chance of occurring together than most other words. They are sort of outliers in the sense they have a greater chance of them occurring out of all the possible combinations.\n",
    "\n",
    "The higher the pmi is between two words, the stronger the collocation is. This collocation between fellow citizens is quite strong as the frequently occur together as there are 61 occurrences of them occurring together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
